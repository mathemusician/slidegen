# AI Module - Local Text Classification

This module provides client-side text embedding capabilities using ONNX Runtime Web.

## Overview

The AI infrastructure is ready but **not yet integrated** into the main application. It will be used later to classify lyric lines (headers vs. lyrics content).

## Components

### `model.js`
- Loads and runs the `all-MiniLM-L6-v2` sentence embedding model
- Provides `embedText(text)` function that returns 384-dimensional Float32Array embeddings
- Includes automatic caching to avoid recomputing embeddings
- Lazy loads ONNX Runtime Web only when first called

### `cosine.js`
- Vector math utilities
- `cosineSimilarity(a, b)` - Computes similarity between two vectors
- `normalize(vec)` - Normalizes a vector to unit length

### `model.test.js`
- Unit tests for the AI module
- Gracefully skips tests when model files are not present

## Usage Example

```javascript
// This code is for future integration - do NOT run yet

import { embedText } from '@/src/ai/model';
import { cosineSimilarity } from '@/src/ai/cosine';

// Embed text
const embedding1 = await embedText("This is a header");
const embedding2 = await embedText("This is some lyrics content");

// Compare similarity
const similarity = cosineSimilarity(embedding1, embedding2);
console.log('Similarity:', similarity); // Value between -1 and 1
```

## Setup Requirements

### 1. Download Model Files

The model files are not included in the repository. Download them from:

**Hugging Face:** https://huggingface.co/onnx-community/all-MiniLM-L6-v2

Place `model.onnx` in: `/public/models/all-MiniLM-L6-v2/`

See `/public/models/all-MiniLM-L6-v2/README.md` for detailed instructions.

### 2. Model Specifications

- **Model:** all-MiniLM-L6-v2
- **Embedding Dimension:** 384
- **Max Sequence Length:** 128 tokens
- **File Size:** ~23 MB
- **Runtime:** Browser-based (WASM)

## Performance Characteristics

- **First Load:** 2-5 seconds (loads ONNX Runtime + model)
- **Subsequent Embeddings:** <50ms per text
- **Caching:** Repeated texts return instantly from cache
- **Memory:** ~50 MB (model + runtime)

## Integration Status

ðŸš§ **Not yet integrated** - This module is infrastructure only.

The AI features will be activated in a future update when:
- The model files are downloaded and available
- The classification logic is implemented
- UI controls are added

## Testing

Run unit tests with:

```bash
npm test
```

Tests will skip gracefully if model files are not present.

## Browser Compatibility

- âœ… Chrome/Edge 90+
- âœ… Firefox 89+
- âœ… Safari 14+
- âœ… Works on mobile browsers

## Technical Notes

### Lazy Loading

The ONNX Runtime and model are loaded dynamically only when `embedText()` is first called. This ensures:
- No impact on initial page load
- No network requests until needed
- Vercel free tier compatibility

### Tokenization

The current implementation uses a simplified tokenizer. For production use, consider:
- Using the official BERT tokenizer from Hugging Face
- Adding `tokenizer.json` and `vocab.txt` to the model directory
- Implementing proper subword tokenization

### Caching Strategy

Embeddings are cached using a `Map` with normalized text as keys. The cache:
- Persists for the browser session
- Can be cleared with `clearCache()`
- Automatically manages memory
